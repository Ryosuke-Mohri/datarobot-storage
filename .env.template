# Refer to https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#create-a-datarobot-api-key
# and https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#retrieve-the-api-endpoint
# Can be deleted on a DataRobot codespace
DATAROBOT_API_TOKEN=
DATAROBOT_ENDPOINT=

# Required, unless logged in to pulumi cloud. Choose your own alphanumeric passphrase to be used for encrypting pulumi config
PULUMI_CONFIG_PASSPHRASE=

# Required: Random string for Web application security. We recommend a long password generated securely such as:
# `python -c "import os, binascii; print(binascii.hexlify(os.urandom(64)).decode('utf-8'))"`
SESSION_SECRET_KEY=

# Optional: If empty, a new use case will be created
DATAROBOT_DEFAULT_USE_CASE=

# Optional: See README instructions for getting Google and Box OAuth Apps
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=

BOX_CLIENT_ID=
BOX_CLIENT_SECRET=

# LLM Configuration:
# Talk to My Docs supports multiple flexible LLM options including:
# - LLM Blueprint with LLM Gateway (default)
# - LLM Blueprint with an External LLM
# - Registered model such as an NVIDIA NIM
# - Already Deployed Text Generation model in DataRobot
#
# You can edit the LLM configuration by manually changing which configuration is
# active (recommended option).
# Simply run `ln -sf ../configurations/<chosen_configuration> infra/infra/llm.py`
#
# If you want to do it dynamically however, you can also set it as a configuration value with:
# INFRA_ENABLE_LLM=<chosen_configuration>
# from the list of options in the infra/configurations/llm folder
# Here are some examples of each of those configuration using the dynamic option described above:

# If you want to use an LLM Blueprint with the LLM Gateway (default)
# uncomment this:
# INFRA_ENABLE_LLM=blueprint_with_llm_gateway.py

# If you want to choose an existing LLM Deployment in DataRobot
# uncomment and configure these:
# TEXTGEN_DEPLOYMENT_ID=<your_deployment_id>
# INFRA_ENABLE_LLM=deployed_llm.py

# If you want to use a Registered Model with an LLM Blueprint
# like an NVIDIA NIM. This also shows how you can adjust the timeout
# in case getting a GPU takes a long time:
# DATAROBOT_TIMEOUT_MINUTES=120
# TEXTGEN_REGISTERED_MODEL_ID='<Your Registered Model ID>'
# INFRA_ENABLE_LLM=registered_model.py

# If you want to configure an LLM with an external LLM provider
# like Azure, Bedrock, Anthropic, or VertexAI (or all 4). Here we provide
# an Azure AI example, see:
# https://docs.datarobot.com/en/docs/gen-ai/playground-tools/deploy-llm.html
# for details on other providers and details:
# INFRA_ENABLE_LLM=blueprint_with_external_llm.py
# LLM_DEFAULT_MODEL="azure/gpt-4o"
# OPENAI_API_VERSION='2024-08-01-preview'
# OPENAI_API_BASE='https://<your_custom_endpoint>.openai.azure.com'
# OPENAI_API_DEPLOYMENT_ID='<your deployment_id>'
# OPENAI_API_KEY='<your_api_key>'

# Custom Execution Environments:
# For deploying with custom execution environments (e.g., in restricted network environments),
# you can specify pre-configured execution environment IDs.
# Contact your DataRobot administrator for execution environment setup and IDs.
#
# Web Application Execution Environment:
# DATAROBOT_WEB_APP_EXECUTION_ENVIRONMENT_ID=<your-web-execution-environment-id>
#
# Agent Execution Environment:
# DATAROBOT_AGENT_EXECUTION_ENVIRONMENT_ID=<your-agent-execution-environment-id>
